{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chieuDai</th>\n",
       "      <th>chieuNgang</th>\n",
       "      <th>dienTich</th>\n",
       "      <th>Gia/m2</th>\n",
       "      <th>Phongngu</th>\n",
       "      <th>SoTang</th>\n",
       "      <th>PhongTam</th>\n",
       "      <th>Loai</th>\n",
       "      <th>GiayTo</th>\n",
       "      <th>TinhTrangNoiThat</th>\n",
       "      <th>Phuong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>65.0</td>\n",
       "      <td>1570.416667</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nhà ngõ, hẻm</td>\n",
       "      <td>đã có sổ</td>\n",
       "      <td>nội thất đầy đủ</td>\n",
       "      <td>phường bình hưng hoà b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3465.416667</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>nhà ngõ, hẻm</td>\n",
       "      <td>đã có sổ</td>\n",
       "      <td>nội thất cao cấp</td>\n",
       "      <td>phường bình trị đông a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2777.916667</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nhà mặt phố, mặt tiền</td>\n",
       "      <td>đã có sổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phường an lạc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3350.833333</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>nhà ngõ, hẻm</td>\n",
       "      <td>đã có sổ</td>\n",
       "      <td>nội thất cao cấp</td>\n",
       "      <td>phường bình trị đông a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.124038</td>\n",
       "      <td>8.124038</td>\n",
       "      <td>66.0</td>\n",
       "      <td>3775.416667</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>nhà ngõ, hẻm</td>\n",
       "      <td>đã có sổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>phường bình hưng hòa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    chieuDai  chieuNgang  dienTich       Gia/m2  Phongngu  SoTang  PhongTam  \\\n",
       "0  16.000000    4.000000      65.0  1570.416667         2     2.0       2.0   \n",
       "1  15.000000    4.000000      60.0  3465.416667         3     3.0       3.0   \n",
       "2  10.000000    4.000000      72.0  2777.916667         2     2.0       2.0   \n",
       "3  18.000000    4.000000      72.0  3350.833333         5     4.0       5.0   \n",
       "4   8.124038    8.124038      66.0  3775.416667         4     4.0       4.0   \n",
       "\n",
       "                    Loai    GiayTo  TinhTrangNoiThat                  Phuong  \n",
       "0           nhà ngõ, hẻm  đã có sổ   nội thất đầy đủ  phường bình hưng hoà b  \n",
       "1           nhà ngõ, hẻm  đã có sổ  nội thất cao cấp  phường bình trị đông a  \n",
       "2  nhà mặt phố, mặt tiền  đã có sổ               NaN           phường an lạc  \n",
       "3           nhà ngõ, hẻm  đã có sổ  nội thất cao cấp  phường bình trị đông a  \n",
       "4           nhà ngõ, hẻm  đã có sổ               NaN    phường bình hưng hòa  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('dataAfterCleaningBinhTan1.csv')\n",
    "data = data.drop_duplicates()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['Loai', 'GiayTo', 'TinhTrangNoiThat', 'Phuong']\n",
    "number_columns = ['chieuDai', 'chieuNgang', 'dienTich', 'Phongngu', 'PhongTam', 'SoTang']\n",
    "for feature in categorical_columns:\n",
    "    data[feature] = data[feature].fillna('unknown')\n",
    "X_raw = data[ number_columns + categorical_columns]\n",
    "y = data['Gia/m2'].to_numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tung\\fourth_year\\DataScience\\project\\venvProject\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['input_layer']. Received: the structure of inputs=*\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 14696048.0000\n",
      "Epoch 2/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1329030.7500\n",
      "Epoch 3/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 949193.6875\n",
      "Epoch 4/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 848951.6875\n",
      "Epoch 5/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 1023071.2500\n",
      "Epoch 6/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 976us/step - loss: 1066624.6250\n",
      "Epoch 7/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 935573.1875\n",
      "Epoch 8/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 763505.3125\n",
      "Epoch 9/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 894239.9375\n",
      "Epoch 10/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step - loss: 797965.6250\n",
      "Epoch 11/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 981us/step - loss: 797066.5000\n",
      "Epoch 12/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 955us/step - loss: 753575.2500\n",
      "Epoch 13/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 766143.6875\n",
      "Epoch 14/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 826301.9375\n",
      "Epoch 15/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 982us/step - loss: 828058.8750\n",
      "Epoch 16/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 658768.5000\n",
      "Epoch 17/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 773183.1875\n",
      "Epoch 18/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 744982.7500\n",
      "Epoch 19/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 748141.8125\n",
      "Epoch 20/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 994us/step - loss: 700870.9375\n",
      "Epoch 21/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973us/step - loss: 749313.5000\n",
      "Epoch 22/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 977us/step - loss: 778328.5625\n",
      "Epoch 23/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 995us/step - loss: 738478.0625\n",
      "Epoch 24/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step - loss: 672702.0000\n",
      "Epoch 25/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 989us/step - loss: 665965.8750\n",
      "Epoch 26/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 964us/step - loss: 776427.2500\n",
      "Epoch 27/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 688601.1250\n",
      "Epoch 28/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - loss: 637424.5625\n",
      "Epoch 29/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 936us/step - loss: 669775.8125\n",
      "Epoch 30/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 654067.8125\n",
      "Epoch 31/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 963us/step - loss: 684901.2500\n",
      "Epoch 32/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 960us/step - loss: 691532.8750\n",
      "Epoch 33/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 599031.2500\n",
      "Epoch 34/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 661106.0625\n",
      "Epoch 35/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 662630.3125\n",
      "Epoch 36/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 723985.6875\n",
      "Epoch 37/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - loss: 654644.2500\n",
      "Epoch 38/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 729449.4375\n",
      "Epoch 39/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 644482.1875\n",
      "Epoch 40/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 949us/step - loss: 664431.6875\n",
      "Epoch 41/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 986us/step - loss: 651245.1250\n",
      "Epoch 42/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 618949.1875\n",
      "Epoch 43/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 600230.0000\n",
      "Epoch 44/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 969us/step - loss: 688417.1875\n",
      "Epoch 45/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 959us/step - loss: 701664.8125\n",
      "Epoch 46/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 983us/step - loss: 681940.5625\n",
      "Epoch 47/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 657368.8125\n",
      "Epoch 48/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 597703.1250\n",
      "Epoch 49/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 583281.6875\n",
      "Epoch 50/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 626239.5625\n",
      "Epoch 51/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 749963.1250\n",
      "Epoch 52/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 626844.5000\n",
      "Epoch 53/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 650493.1875\n",
      "Epoch 54/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 602645.1875\n",
      "Epoch 55/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 615134.8125\n",
      "Epoch 56/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 591324.4375\n",
      "Epoch 57/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 673310.9375\n",
      "Epoch 58/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 979us/step - loss: 607280.4375\n",
      "Epoch 59/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 636299.0625\n",
      "Epoch 60/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 656684.8750\n",
      "Epoch 61/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 686750.9375\n",
      "Epoch 62/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 609128.6875\n",
      "Epoch 63/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 602550.5000\n",
      "Epoch 64/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 747481.5000\n",
      "Epoch 65/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 653398.4375\n",
      "Epoch 66/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 582602.9375\n",
      "Epoch 67/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 624089.0000\n",
      "Epoch 68/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 591001.4375\n",
      "Epoch 69/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 591815.6875\n",
      "Epoch 70/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 656966.2500\n",
      "Epoch 71/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 618026.3750  \n",
      "Epoch 72/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 603032.4375\n",
      "Epoch 73/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 543688.0625\n",
      "Epoch 74/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 565206.0625\n",
      "Epoch 75/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 696327.4375\n",
      "Epoch 76/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 672750.5000\n",
      "Epoch 77/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 641191.6250\n",
      "Epoch 78/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 537889.0625\n",
      "Epoch 79/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 681326.5625\n",
      "Epoch 80/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 621080.8750 \n",
      "Epoch 81/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 605402.1250\n",
      "Epoch 82/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 739966.6250\n",
      "Epoch 83/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 567551.7500\n",
      "Epoch 84/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 663732.8750\n",
      "Epoch 85/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 648960.3125\n",
      "Epoch 86/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 586171.6250\n",
      "Epoch 87/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985us/step - loss: 600465.3125\n",
      "Epoch 88/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 582856.8125\n",
      "Epoch 89/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 574172.5000\n",
      "Epoch 90/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 613519.3125\n",
      "Epoch 91/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 662083.2500\n",
      "Epoch 92/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 576171.1875\n",
      "Epoch 93/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 616024.4375  \n",
      "Epoch 94/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 596687.6875\n",
      "Epoch 95/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 589558.8125\n",
      "Epoch 96/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 561494.1250\n",
      "Epoch 97/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 609737.8750\n",
      "Epoch 98/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 596009.8750\n",
      "Epoch 99/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 582833.4375\n",
      "Epoch 100/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 555594.8750\n",
      "Epoch 101/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 659861.5000\n",
      "Epoch 102/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 598275.1875\n",
      "Epoch 103/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 988us/step - loss: 614778.3750\n",
      "Epoch 104/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 704173.0000\n",
      "Epoch 105/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 565111.3125\n",
      "Epoch 106/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 593059.6875\n",
      "Epoch 107/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 997us/step - loss: 617498.5000\n",
      "Epoch 108/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 629116.5625\n",
      "Epoch 109/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 616217.3125\n",
      "Epoch 110/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000us/step - loss: 553146.4375\n",
      "Epoch 111/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 542917.4375\n",
      "Epoch 112/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 577414.5000\n",
      "Epoch 113/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 560103.0625\n",
      "Epoch 114/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998us/step - loss: 641974.4375\n",
      "Epoch 115/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 537963.7500\n",
      "Epoch 116/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 589047.1875\n",
      "Epoch 117/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 996us/step - loss: 570835.6250\n",
      "Epoch 118/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 592290.9375\n",
      "Epoch 119/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 544430.2500\n",
      "Epoch 120/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 978us/step - loss: 620207.6250\n",
      "Epoch 121/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 564198.0625\n",
      "Epoch 122/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 541016.3125\n",
      "Epoch 123/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - loss: 534340.5625\n",
      "Epoch 124/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 571378.6250\n",
      "Epoch 125/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 501225.7500\n",
      "Epoch 126/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 531531.0000\n",
      "Epoch 127/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 568780.9375\n",
      "Epoch 128/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 550797.7500\n",
      "Epoch 129/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 552113.3750\n",
      "Epoch 130/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 552679.3750\n",
      "Epoch 131/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 999us/step - loss: 621868.1250\n",
      "Epoch 132/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 517173.2500\n",
      "Epoch 133/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 962us/step - loss: 521923.3125\n",
      "Epoch 134/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 553667.8125\n",
      "Epoch 135/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 564377.8125\n",
      "Epoch 136/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 591535.1250\n",
      "Epoch 137/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 602578.6875\n",
      "Epoch 138/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 507507.5938\n",
      "Epoch 139/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 493649.9688\n",
      "Epoch 140/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 538826.5625\n",
      "Epoch 141/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 503109.8438\n",
      "Epoch 142/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 521461.5312\n",
      "Epoch 143/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 553729.4375\n",
      "Epoch 144/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 537640.1875\n",
      "Epoch 145/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 987us/step - loss: 563851.0625\n",
      "Epoch 146/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 523991.7812\n",
      "Epoch 147/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 499893.8125\n",
      "Epoch 148/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 529397.3125\n",
      "Epoch 149/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 545452.9375  \n",
      "Epoch 150/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 532040.2500\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=1609)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(X_train[number_columns])\n",
    "X_numerical_test = scaler.transform(X_test[number_columns])\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_categorical = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_categorical_test = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "X_combined_train = np.concatenate((X_numerical, X_categorical), axis=1)\n",
    "X_combined_test = np.concatenate((X_numerical_test, X_categorical_test), axis=1)\n",
    "\n",
    "input_layer = Input(shape=(X_combined_train.shape[1],), name='input_layer')  \n",
    "x1 = Dense(128, activation='relu')(input_layer)\n",
    "x1 = Dense(64, activation='relu')(x1)\n",
    "merged = Dense(32, activation='relu')(x1)\n",
    "merged = Dense(16, activation='relu')(merged)\n",
    "output = Dense(1)(merged)\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit(X_combined_train, y_train, epochs=150, batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_combined_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tung\\fourth_year\\DataScience\\project\\venvProject\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 15200164.85714| val_0_mse: 15372432.02814|  0:00:00s\n",
      "epoch 1  | loss: 15315182.14286| val_0_mse: 15338604.7448|  0:00:00s\n",
      "epoch 2  | loss: 15161773.14286| val_0_mse: 15295758.12037|  0:00:00s\n",
      "epoch 3  | loss: 15185023.0| val_0_mse: 15240179.06173|  0:00:00s\n",
      "epoch 4  | loss: 15150373.57143| val_0_mse: 15172198.56825|  0:00:01s\n",
      "epoch 5  | loss: 15132711.28571| val_0_mse: 15101571.59817|  0:00:01s\n",
      "epoch 6  | loss: 15031307.71429| val_0_mse: 15034942.75722|  0:00:01s\n",
      "epoch 7  | loss: 14896775.57143| val_0_mse: 14966135.96618|  0:00:01s\n",
      "epoch 8  | loss: 14834217.42857| val_0_mse: 14892085.41507|  0:00:02s\n",
      "epoch 9  | loss: 14753202.28571| val_0_mse: 14810913.47158|  0:00:02s\n",
      "epoch 10 | loss: 14711228.14286| val_0_mse: 14728642.92611|  0:00:02s\n",
      "epoch 11 | loss: 14517780.28571| val_0_mse: 14616454.62446|  0:00:02s\n",
      "epoch 12 | loss: 14319542.28571| val_0_mse: 14489336.06544|  0:00:03s\n",
      "epoch 13 | loss: 14132019.71429| val_0_mse: 14357294.64467|  0:00:03s\n",
      "epoch 14 | loss: 13966485.14286| val_0_mse: 14243213.26853|  0:00:03s\n",
      "epoch 15 | loss: 13720773.71429| val_0_mse: 14060096.1577|  0:00:03s\n",
      "epoch 16 | loss: 13670119.57143| val_0_mse: 13808994.09409|  0:00:04s\n",
      "epoch 17 | loss: 13438542.14286| val_0_mse: 13611953.6531|  0:00:04s\n",
      "epoch 18 | loss: 13204688.71429| val_0_mse: 13300400.62577|  0:00:04s\n",
      "epoch 19 | loss: 12891122.14286| val_0_mse: 12915493.36261|  0:00:04s\n",
      "epoch 20 | loss: 12806969.71429| val_0_mse: 12748920.45713|  0:00:05s\n",
      "epoch 21 | loss: 12494353.71429| val_0_mse: 12569224.07444|  0:00:05s\n",
      "epoch 22 | loss: 12167699.42857| val_0_mse: 12543333.79062|  0:00:05s\n",
      "epoch 23 | loss: 12047201.85714| val_0_mse: 12418214.68091|  0:00:05s\n",
      "epoch 24 | loss: 11770387.14286| val_0_mse: 12029395.56389|  0:00:05s\n",
      "epoch 25 | loss: 11505316.71429| val_0_mse: 11580101.40382|  0:00:06s\n",
      "epoch 26 | loss: 11256284.28571| val_0_mse: 11064959.55411|  0:00:06s\n",
      "epoch 27 | loss: 10919397.28571| val_0_mse: 10960323.98039|  0:00:06s\n",
      "epoch 28 | loss: 10685572.14286| val_0_mse: 10583155.89593|  0:00:06s\n",
      "epoch 29 | loss: 10467729.14286| val_0_mse: 10096375.70933|  0:00:07s\n",
      "epoch 30 | loss: 10102305.71429| val_0_mse: 9836650.85186|  0:00:07s\n",
      "epoch 31 | loss: 9712264.85714| val_0_mse: 9963011.30595|  0:00:07s\n",
      "epoch 32 | loss: 9599001.0| val_0_mse: 9581178.60149|  0:00:07s\n",
      "epoch 33 | loss: 9247438.85714| val_0_mse: 9125951.31528|  0:00:08s\n",
      "epoch 34 | loss: 9076035.71429| val_0_mse: 9125695.28324|  0:00:08s\n",
      "epoch 35 | loss: 8856623.71429| val_0_mse: 8575386.83917|  0:00:08s\n",
      "epoch 36 | loss: 8581401.42857| val_0_mse: 8582392.31554|  0:00:08s\n",
      "epoch 37 | loss: 8250445.28571| val_0_mse: 8423122.85074|  0:00:09s\n",
      "epoch 38 | loss: 7960360.85714| val_0_mse: 8108307.06964|  0:00:09s\n",
      "epoch 39 | loss: 7693130.21429| val_0_mse: 8193619.57894|  0:00:09s\n",
      "epoch 40 | loss: 7363541.57143| val_0_mse: 7397198.77754|  0:00:09s\n",
      "epoch 41 | loss: 7061182.57143| val_0_mse: 7285898.52673|  0:00:09s\n",
      "epoch 42 | loss: 6832653.35714| val_0_mse: 6286882.50014|  0:00:10s\n",
      "epoch 43 | loss: 6532114.71429| val_0_mse: 6047955.91592|  0:00:10s\n",
      "epoch 44 | loss: 6238738.64286| val_0_mse: 5790900.60617|  0:00:10s\n",
      "epoch 45 | loss: 5889566.28571| val_0_mse: 6221225.54079|  0:00:10s\n",
      "epoch 46 | loss: 5718843.85714| val_0_mse: 5784847.37319|  0:00:11s\n",
      "epoch 47 | loss: 5480439.57143| val_0_mse: 5609492.99855|  0:00:11s\n",
      "epoch 48 | loss: 5149294.85714| val_0_mse: 5346158.88915|  0:00:11s\n",
      "epoch 49 | loss: 4943264.78571| val_0_mse: 5472248.64582|  0:00:11s\n",
      "epoch 50 | loss: 4806676.57143| val_0_mse: 5022633.62279|  0:00:12s\n",
      "epoch 51 | loss: 4550915.10714| val_0_mse: 4583429.31368|  0:00:12s\n",
      "epoch 52 | loss: 4429324.60714| val_0_mse: 4349579.66197|  0:00:12s\n",
      "epoch 53 | loss: 4107395.78571| val_0_mse: 3783679.2321|  0:00:12s\n",
      "epoch 54 | loss: 3842436.67857| val_0_mse: 4149980.66565|  0:00:13s\n",
      "epoch 55 | loss: 3613312.21429| val_0_mse: 3445736.85771|  0:00:13s\n",
      "epoch 56 | loss: 3474332.46429| val_0_mse: 4174644.43233|  0:00:13s\n",
      "epoch 57 | loss: 3174060.92857| val_0_mse: 3903812.0894|  0:00:13s\n",
      "epoch 58 | loss: 3007625.71429| val_0_mse: 3716174.12732|  0:00:14s\n",
      "epoch 59 | loss: 2895779.35714| val_0_mse: 3031286.11349|  0:00:14s\n",
      "epoch 60 | loss: 2648425.46429| val_0_mse: 2771194.79936|  0:00:14s\n",
      "epoch 61 | loss: 2562613.53571| val_0_mse: 2076922.93418|  0:00:14s\n",
      "epoch 62 | loss: 2346359.75| val_0_mse: 1567609.66971|  0:00:15s\n",
      "epoch 63 | loss: 2307965.53571| val_0_mse: 1891971.33527|  0:00:15s\n",
      "epoch 64 | loss: 2155316.48214| val_0_mse: 1705697.74775|  0:00:15s\n",
      "epoch 65 | loss: 2043452.89286| val_0_mse: 1846465.64177|  0:00:15s\n",
      "epoch 66 | loss: 1858055.08929| val_0_mse: 2089490.26235|  0:00:16s\n",
      "epoch 67 | loss: 1741041.85714| val_0_mse: 2264220.19324|  0:00:16s\n",
      "epoch 68 | loss: 1589263.17857| val_0_mse: 1366901.32262|  0:00:16s\n",
      "epoch 69 | loss: 1460265.35714| val_0_mse: 1335834.85404|  0:00:16s\n",
      "epoch 70 | loss: 1395353.33929| val_0_mse: 1287611.87239|  0:00:16s\n",
      "epoch 71 | loss: 1288584.57143| val_0_mse: 1377822.96123|  0:00:17s\n",
      "epoch 72 | loss: 1183257.08036| val_0_mse: 1123746.79509|  0:00:17s\n",
      "epoch 73 | loss: 1143623.78571| val_0_mse: 1248284.09108|  0:00:17s\n",
      "epoch 74 | loss: 1069364.54464| val_0_mse: 1077748.57769|  0:00:17s\n",
      "epoch 75 | loss: 1030314.08929| val_0_mse: 985593.52975|  0:00:18s\n",
      "epoch 76 | loss: 1014984.02679| val_0_mse: 1090174.37184|  0:00:18s\n",
      "epoch 77 | loss: 964234.10714| val_0_mse: 1315982.72837|  0:00:18s\n",
      "epoch 78 | loss: 960730.45536| val_0_mse: 1054534.14707|  0:00:18s\n",
      "epoch 79 | loss: 920502.10714| val_0_mse: 1059112.77983|  0:00:19s\n",
      "epoch 80 | loss: 889815.45536| val_0_mse: 893246.30216|  0:00:19s\n",
      "epoch 81 | loss: 857624.91964| val_0_mse: 924877.21308|  0:00:19s\n",
      "epoch 82 | loss: 855400.70536| val_0_mse: 904443.18364|  0:00:19s\n",
      "epoch 83 | loss: 809209.00893| val_0_mse: 931576.05499|  0:00:19s\n",
      "epoch 84 | loss: 802112.83929| val_0_mse: 914271.03674|  0:00:20s\n",
      "epoch 85 | loss: 819740.83036| val_0_mse: 924134.36276|  0:00:20s\n",
      "epoch 86 | loss: 801041.625| val_0_mse: 910013.95139|  0:00:20s\n",
      "epoch 87 | loss: 863657.875| val_0_mse: 904746.49497|  0:00:20s\n",
      "epoch 88 | loss: 836219.13393| val_0_mse: 953558.33974|  0:00:21s\n",
      "epoch 89 | loss: 817894.0625| val_0_mse: 881057.19976|  0:00:21s\n",
      "epoch 90 | loss: 831083.09821| val_0_mse: 951387.80028|  0:00:21s\n",
      "epoch 91 | loss: 806797.08036| val_0_mse: 966031.57516|  0:00:21s\n",
      "epoch 92 | loss: 796593.55357| val_0_mse: 937480.59649|  0:00:22s\n",
      "epoch 93 | loss: 807306.38393| val_0_mse: 895025.51045|  0:00:22s\n",
      "epoch 94 | loss: 811787.0625| val_0_mse: 960431.18699|  0:00:22s\n",
      "epoch 95 | loss: 783796.53571| val_0_mse: 912477.52784|  0:00:22s\n",
      "epoch 96 | loss: 789386.0| val_0_mse: 893771.79171|  0:00:22s\n",
      "epoch 97 | loss: 784749.49107| val_0_mse: 931009.43002|  0:00:23s\n",
      "epoch 98 | loss: 815583.75| val_0_mse: 1349492.96373|  0:00:23s\n",
      "epoch 99 | loss: 824503.41071| val_0_mse: 948832.27455|  0:00:23s\n",
      "epoch 100| loss: 790504.90179| val_0_mse: 1591402.29529|  0:00:23s\n",
      "epoch 101| loss: 806153.35714| val_0_mse: 854459.51858|  0:00:24s\n",
      "epoch 102| loss: 774567.42857| val_0_mse: 854422.13003|  0:00:24s\n",
      "epoch 103| loss: 813346.73214| val_0_mse: 897027.69721|  0:00:24s\n",
      "epoch 104| loss: 784824.00893| val_0_mse: 871792.78856|  0:00:24s\n",
      "epoch 105| loss: 767835.39286| val_0_mse: 888385.28931|  0:00:25s\n",
      "epoch 106| loss: 764289.14286| val_0_mse: 914788.99363|  0:00:25s\n",
      "epoch 107| loss: 714575.85714| val_0_mse: 953525.30465|  0:00:25s\n",
      "epoch 108| loss: 746391.38393| val_0_mse: 894662.90701|  0:00:25s\n",
      "epoch 109| loss: 726883.96429| val_0_mse: 1003937.05481|  0:00:25s\n",
      "epoch 110| loss: 764061.875| val_0_mse: 856750.44356|  0:00:26s\n",
      "epoch 111| loss: 752852.08036| val_0_mse: 890580.10322|  0:00:26s\n",
      "epoch 112| loss: 752458.36607| val_0_mse: 800050.97834|  0:00:26s\n",
      "epoch 113| loss: 711315.52679| val_0_mse: 776679.20746|  0:00:26s\n",
      "epoch 114| loss: 727568.84821| val_0_mse: 816807.41764|  0:00:27s\n",
      "epoch 115| loss: 702683.5625| val_0_mse: 816509.21756|  0:00:27s\n",
      "epoch 116| loss: 711247.1875| val_0_mse: 805757.35512|  0:00:27s\n",
      "epoch 117| loss: 730237.26786| val_0_mse: 840536.817|  0:00:27s\n",
      "epoch 118| loss: 695252.69643| val_0_mse: 776039.66576|  0:00:27s\n",
      "epoch 119| loss: 670383.40179| val_0_mse: 810998.63379|  0:00:28s\n",
      "epoch 120| loss: 694647.15179| val_0_mse: 768666.33539|  0:00:28s\n",
      "epoch 121| loss: 705691.78571| val_0_mse: 781737.65155|  0:00:28s\n",
      "epoch 122| loss: 636937.64732| val_0_mse: 827310.66041|  0:00:28s\n",
      "epoch 123| loss: 635161.05357| val_0_mse: 843392.14385|  0:00:29s\n",
      "epoch 124| loss: 676496.21429| val_0_mse: 767599.1844|  0:00:29s\n",
      "epoch 125| loss: 696369.24107| val_0_mse: 802358.66631|  0:00:29s\n",
      "epoch 126| loss: 696214.79464| val_0_mse: 820664.6288|  0:00:29s\n",
      "epoch 127| loss: 684112.91964| val_0_mse: 820645.97894|  0:00:30s\n",
      "epoch 128| loss: 694415.20536| val_0_mse: 805976.79699|  0:00:30s\n",
      "epoch 129| loss: 665260.75893| val_0_mse: 846377.72623|  0:00:30s\n",
      "epoch 130| loss: 676079.61607| val_0_mse: 820620.55657|  0:00:30s\n",
      "epoch 131| loss: 659321.45536| val_0_mse: 816101.43709|  0:00:30s\n",
      "epoch 132| loss: 659870.49554| val_0_mse: 840316.50402|  0:00:31s\n",
      "epoch 133| loss: 654625.29464| val_0_mse: 864498.2167|  0:00:31s\n",
      "epoch 134| loss: 662887.39286| val_0_mse: 835787.76277|  0:00:31s\n",
      "epoch 135| loss: 669210.60714| val_0_mse: 822016.95874|  0:00:31s\n",
      "epoch 136| loss: 661671.45536| val_0_mse: 765517.16101|  0:00:32s\n",
      "epoch 137| loss: 669910.85268| val_0_mse: 777374.56812|  0:00:32s\n",
      "epoch 138| loss: 661112.76786| val_0_mse: 773007.17279|  0:00:32s\n",
      "epoch 139| loss: 669255.19643| val_0_mse: 791846.20786|  0:00:32s\n",
      "epoch 140| loss: 643400.39732| val_0_mse: 812627.50097|  0:00:33s\n",
      "epoch 141| loss: 647656.53571| val_0_mse: 784099.24218|  0:00:33s\n",
      "epoch 142| loss: 651731.71429| val_0_mse: 851065.31326|  0:00:33s\n",
      "epoch 143| loss: 633536.49107| val_0_mse: 832910.23983|  0:00:33s\n",
      "epoch 144| loss: 655281.04018| val_0_mse: 828018.50216|  0:00:33s\n",
      "epoch 145| loss: 601244.10714| val_0_mse: 777193.69705|  0:00:34s\n",
      "epoch 146| loss: 679134.73214| val_0_mse: 748195.13497|  0:00:34s\n",
      "epoch 147| loss: 610209.24554| val_0_mse: 769273.07709|  0:00:34s\n",
      "epoch 148| loss: 675161.55357| val_0_mse: 827190.91055|  0:00:34s\n",
      "epoch 149| loss: 635499.83036| val_0_mse: 798827.82883|  0:00:35s\n",
      "epoch 150| loss: 654124.27679| val_0_mse: 823734.75519|  0:00:35s\n",
      "epoch 151| loss: 635389.16964| val_0_mse: 838182.30611|  0:00:35s\n",
      "epoch 152| loss: 633801.20982| val_0_mse: 828089.25282|  0:00:35s\n",
      "epoch 153| loss: 621861.33929| val_0_mse: 786258.35587|  0:00:35s\n",
      "epoch 154| loss: 647054.34821| val_0_mse: 785133.3563|  0:00:36s\n",
      "epoch 155| loss: 648055.22321| val_0_mse: 783608.06698|  0:00:36s\n",
      "epoch 156| loss: 627076.13393| val_0_mse: 793774.25583|  0:00:36s\n",
      "epoch 157| loss: 668525.41071| val_0_mse: 817530.78868|  0:00:36s\n",
      "epoch 158| loss: 676730.49554| val_0_mse: 857144.18774|  0:00:37s\n",
      "epoch 159| loss: 657925.03571| val_0_mse: 848518.0306|  0:00:37s\n",
      "epoch 160| loss: 662556.84821| val_0_mse: 874207.12936|  0:00:37s\n",
      "epoch 161| loss: 658969.26786| val_0_mse: 810929.38307|  0:00:37s\n",
      "epoch 162| loss: 635337.88839| val_0_mse: 781633.65833|  0:00:38s\n",
      "epoch 163| loss: 639927.80357| val_0_mse: 834579.33023|  0:00:38s\n",
      "epoch 164| loss: 620552.95536| val_0_mse: 811407.85781|  0:00:38s\n",
      "epoch 165| loss: 623109.09821| val_0_mse: 814080.97618|  0:00:38s\n",
      "epoch 166| loss: 622370.13839| val_0_mse: 804581.4756|  0:00:39s\n",
      "epoch 167| loss: 617068.13839| val_0_mse: 818464.32032|  0:00:39s\n",
      "epoch 168| loss: 609553.10714| val_0_mse: 789066.15044|  0:00:39s\n",
      "epoch 169| loss: 562741.04464| val_0_mse: 809500.37374|  0:00:39s\n",
      "epoch 170| loss: 611700.63839| val_0_mse: 803009.80693|  0:00:39s\n",
      "epoch 171| loss: 648227.04464| val_0_mse: 780544.76089|  0:00:40s\n",
      "epoch 172| loss: 615951.22321| val_0_mse: 797368.82559|  0:00:40s\n",
      "epoch 173| loss: 628771.6875| val_0_mse: 808464.5346|  0:00:40s\n",
      "epoch 174| loss: 609475.22321| val_0_mse: 818690.92288|  0:00:40s\n",
      "epoch 175| loss: 594940.28571| val_0_mse: 859132.73267|  0:00:41s\n",
      "epoch 176| loss: 598098.12946| val_0_mse: 804731.11829|  0:00:41s\n",
      "epoch 177| loss: 625099.91518| val_0_mse: 810155.08647|  0:00:41s\n",
      "epoch 178| loss: 616903.64732| val_0_mse: 808067.92873|  0:00:41s\n",
      "epoch 179| loss: 604473.67411| val_0_mse: 790189.00748|  0:00:41s\n",
      "epoch 180| loss: 597554.14732| val_0_mse: 807613.24685|  0:00:42s\n",
      "epoch 181| loss: 611502.25893| val_0_mse: 831327.57112|  0:00:42s\n",
      "epoch 182| loss: 605485.11161| val_0_mse: 835687.41128|  0:00:42s\n",
      "epoch 183| loss: 611226.37054| val_0_mse: 837074.14043|  0:00:42s\n",
      "epoch 184| loss: 613082.02679| val_0_mse: 850298.68506|  0:00:43s\n",
      "epoch 185| loss: 558226.35268| val_0_mse: 836001.02006|  0:00:43s\n",
      "epoch 186| loss: 593581.76786| val_0_mse: 837543.27661|  0:00:43s\n",
      "epoch 187| loss: 585550.38393| val_0_mse: 845203.27626|  0:00:43s\n",
      "epoch 188| loss: 575489.25893| val_0_mse: 819020.54884|  0:00:43s\n",
      "epoch 189| loss: 598101.08036| val_0_mse: 833347.00581|  0:00:44s\n",
      "epoch 190| loss: 584540.70536| val_0_mse: 853106.0837|  0:00:44s\n",
      "epoch 191| loss: 588208.67857| val_0_mse: 847935.66623|  0:00:44s\n",
      "epoch 192| loss: 581694.35714| val_0_mse: 846788.18457|  0:00:44s\n",
      "epoch 193| loss: 606717.19643| val_0_mse: 853706.99764|  0:00:45s\n",
      "epoch 194| loss: 555464.06696| val_0_mse: 867714.66778|  0:00:45s\n",
      "epoch 195| loss: 603921.54464| val_0_mse: 864311.87109|  0:00:45s\n",
      "epoch 196| loss: 585817.58482| val_0_mse: 853274.82357|  0:00:45s\n",
      "epoch 197| loss: 528209.21875| val_0_mse: 839183.40445|  0:00:46s\n",
      "epoch 198| loss: 571182.62054| val_0_mse: 867501.61815|  0:00:46s\n",
      "epoch 199| loss: 581140.58482| val_0_mse: 855432.75075|  0:00:46s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 146 and best_val_0_mse = 748195.13497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tung\\fourth_year\\DataScience\\project\\venvProject\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=1609)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical_train = scaler.fit_transform(X_train[number_columns])\n",
    "X_numerical_test = scaler.transform(X_test[number_columns])\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_categorical_train = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_categorical_test = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "X_train_combined = np.concatenate((X_numerical_train, X_categorical_train), axis=1)\n",
    "X_test_combined = np.concatenate((X_numerical_test, X_categorical_test), axis=1)\n",
    "\n",
    "model = TabNetRegressor()\n",
    "model.fit(X_train_combined, y_train, \n",
    "          eval_set=[(X_test_combined, y_test)],\n",
    "          max_epochs=200, \n",
    "          patience=60, \n",
    "          batch_size=512,\n",
    "          )\n",
    "predictions = model.predict(X_test_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tung\\fourth_year\\DataScience\\project\\venvProject\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['wide_input', 'deep_numerical_input', 'deep_categorical_input']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 14586981.0000\n",
      "Epoch 2/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2365422.7500\n",
      "Epoch 3/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1118222.8750\n",
      "Epoch 4/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 986221.5625 \n",
      "Epoch 5/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 900659.1875\n",
      "Epoch 6/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 821059.6875\n",
      "Epoch 7/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 817893.9375\n",
      "Epoch 8/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 771896.9375\n",
      "Epoch 9/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 746175.3750\n",
      "Epoch 10/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 797644.1875\n",
      "Epoch 11/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 837994.6875\n",
      "Epoch 12/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 784568.2500\n",
      "Epoch 13/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 747694.3750\n",
      "Epoch 14/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 768446.6250\n",
      "Epoch 15/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 791420.6250\n",
      "Epoch 16/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 694815.0625\n",
      "Epoch 17/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 720026.5000\n",
      "Epoch 18/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 943192.0000 \n",
      "Epoch 19/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 699338.1875\n",
      "Epoch 20/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 724967.6250\n",
      "Epoch 21/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 740716.6250\n",
      "Epoch 22/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 776232.7500\n",
      "Epoch 23/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 823161.1250\n",
      "Epoch 24/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 742457.3750\n",
      "Epoch 25/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 715229.8125\n",
      "Epoch 26/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 717872.3750\n",
      "Epoch 27/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 695065.2500\n",
      "Epoch 28/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 751184.0000\n",
      "Epoch 29/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 707957.0000\n",
      "Epoch 30/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 687947.7500\n",
      "Epoch 31/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 736709.7500\n",
      "Epoch 32/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 733859.3125\n",
      "Epoch 33/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 838055.8750\n",
      "Epoch 34/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 693262.6875\n",
      "Epoch 35/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 769935.5000\n",
      "Epoch 36/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 756103.0625\n",
      "Epoch 37/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 663939.5625\n",
      "Epoch 38/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 738168.8750\n",
      "Epoch 39/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 754093.6250\n",
      "Epoch 40/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 703590.7500\n",
      "Epoch 41/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 712181.8125\n",
      "Epoch 42/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 715508.8750\n",
      "Epoch 43/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 681216.0000\n",
      "Epoch 44/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 652062.9375\n",
      "Epoch 45/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 655967.9375\n",
      "Epoch 46/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 682736.1875\n",
      "Epoch 47/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 862065.5000\n",
      "Epoch 48/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 606436.5625\n",
      "Epoch 49/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 862940.3125\n",
      "Epoch 50/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 759006.6875\n",
      "Epoch 51/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 719429.6250\n",
      "Epoch 52/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 681266.1875\n",
      "Epoch 53/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 784140.1250\n",
      "Epoch 54/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 713120.1875\n",
      "Epoch 55/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 814030.2500\n",
      "Epoch 56/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 647276.8750\n",
      "Epoch 57/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 706147.8125\n",
      "Epoch 58/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 695714.7500\n",
      "Epoch 59/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 730578.5625\n",
      "Epoch 60/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 710581.0625\n",
      "Epoch 61/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 656060.3125\n",
      "Epoch 62/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 737872.6250\n",
      "Epoch 63/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 642781.0625\n",
      "Epoch 64/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 680438.6250\n",
      "Epoch 65/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 677923.1875\n",
      "Epoch 66/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 687822.6875\n",
      "Epoch 67/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 648539.0000\n",
      "Epoch 68/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 729536.4375\n",
      "Epoch 69/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 749092.8125\n",
      "Epoch 70/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 643902.8125\n",
      "Epoch 71/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 665359.6875\n",
      "Epoch 72/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 658734.1875\n",
      "Epoch 73/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 695082.6875\n",
      "Epoch 74/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 674241.2500\n",
      "Epoch 75/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 723487.6875\n",
      "Epoch 76/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 689562.2500\n",
      "Epoch 77/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 665353.8125\n",
      "Epoch 78/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 706946.1250\n",
      "Epoch 79/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 650635.1875\n",
      "Epoch 80/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 836910.5000\n",
      "Epoch 81/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 706744.0000\n",
      "Epoch 82/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 682830.0000\n",
      "Epoch 83/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 748517.2500\n",
      "Epoch 84/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 719442.9375\n",
      "Epoch 85/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 724646.3125\n",
      "Epoch 86/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 694360.5000\n",
      "Epoch 87/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 685416.8125\n",
      "Epoch 88/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 645003.7500\n",
      "Epoch 89/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 662454.6250\n",
      "Epoch 90/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 680083.6250\n",
      "Epoch 91/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 642018.8125\n",
      "Epoch 92/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 653519.9375\n",
      "Epoch 93/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 720282.9375\n",
      "Epoch 94/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 717243.0625\n",
      "Epoch 95/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 690505.0625\n",
      "Epoch 96/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 669764.6250\n",
      "Epoch 97/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 752205.7500\n",
      "Epoch 98/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 602086.6250\n",
      "Epoch 99/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 722331.9375\n",
      "Epoch 100/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 658922.6875\n",
      "Epoch 101/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 871710.3750\n",
      "Epoch 102/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 652330.5000\n",
      "Epoch 103/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 688853.3125\n",
      "Epoch 104/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 712250.7500\n",
      "Epoch 105/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 722481.9375\n",
      "Epoch 106/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 696402.2500\n",
      "Epoch 107/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 621937.5000\n",
      "Epoch 108/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 653490.9375\n",
      "Epoch 109/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 677287.4375\n",
      "Epoch 110/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 624841.5625\n",
      "Epoch 111/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 743813.8125\n",
      "Epoch 112/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 648975.1250\n",
      "Epoch 113/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 689909.6250\n",
      "Epoch 114/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 650361.7500\n",
      "Epoch 115/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 712304.6875\n",
      "Epoch 116/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 595191.6875\n",
      "Epoch 117/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 661661.3125\n",
      "Epoch 118/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 693029.8750\n",
      "Epoch 119/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 608949.6875\n",
      "Epoch 120/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 607875.6250\n",
      "Epoch 121/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 730919.4375\n",
      "Epoch 122/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 671562.0000\n",
      "Epoch 123/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 622256.3125\n",
      "Epoch 124/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 612258.2500\n",
      "Epoch 125/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 668087.1250\n",
      "Epoch 126/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 743035.7500\n",
      "Epoch 127/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 636017.1875\n",
      "Epoch 128/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 677194.0000\n",
      "Epoch 129/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 661360.5000\n",
      "Epoch 130/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 663833.0625\n",
      "Epoch 131/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 674205.0000\n",
      "Epoch 132/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 647239.0000\n",
      "Epoch 133/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 634551.4375\n",
      "Epoch 134/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 780062.2500\n",
      "Epoch 135/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 597392.7500\n",
      "Epoch 136/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 683614.5625\n",
      "Epoch 137/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 624567.8750\n",
      "Epoch 138/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 601118.5000\n",
      "Epoch 139/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 602583.7500\n",
      "Epoch 140/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 666767.5000\n",
      "Epoch 141/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 625394.1875\n",
      "Epoch 142/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 705763.1875\n",
      "Epoch 143/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 601462.8125\n",
      "Epoch 144/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 615874.6250\n",
      "Epoch 145/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 612140.6250\n",
      "Epoch 146/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 713007.0625\n",
      "Epoch 147/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 627674.1875\n",
      "Epoch 148/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 598507.5000\n",
      "Epoch 149/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 781485.0625\n",
      "Epoch 150/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 655859.9375\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, random_state=1609)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical_train = scaler.fit_transform(X_train[number_columns])\n",
    "X_numerical_test = scaler.transform(X_test[number_columns])\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_categorical_train = encoder.fit_transform(X_train[categorical_columns])\n",
    "X_categorical_test = encoder.transform(X_test[categorical_columns])\n",
    "\n",
    "wide_input = Input(shape=(X_categorical_train.shape[1],), name='wide_input')\n",
    "wide_output = Dense(1)(wide_input)\n",
    "\n",
    "deep_numerical_input = Input(shape=(X_numerical_train.shape[1],), name='deep_numerical_input')\n",
    "deep_x = Dense(128, activation='relu')(deep_numerical_input)\n",
    "deep_x = Dense(64, activation='relu')(deep_x)\n",
    "deep_x = Dense(32, activation='relu')(deep_x)\n",
    "\n",
    "deep_categorical_input = Input(shape=(X_categorical_train.shape[1],), name='deep_categorical_input')\n",
    "deep_y = Dense(128, activation='relu')(deep_categorical_input)\n",
    "deep_y = Dense(64, activation='relu')(deep_y)\n",
    "deep_y = Dense(32, activation='relu')(deep_y)\n",
    "\n",
    "merged = Concatenate()([wide_output, deep_x, deep_y])\n",
    "output = Dense(1)(merged)\n",
    "\n",
    "model = Model(inputs=[wide_input, deep_numerical_input, deep_categorical_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit([X_categorical_train, X_numerical_train, X_categorical_train], y_train, epochs=150, batch_size=32)\n",
    "\n",
    "predictions = model.predict([X_categorical_test, X_numerical_test, X_categorical_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tung\\fourth_year\\DataScience\\project\\venvProject\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['numerical_input', 'categorical_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 15033000.0000\n",
      "Epoch 2/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6338099.5000\n",
      "Epoch 3/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1340044.0000\n",
      "Epoch 4/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1250731.7500\n",
      "Epoch 5/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1060173.2500\n",
      "Epoch 6/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1108613.5000\n",
      "Epoch 7/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 991487.0000 \n",
      "Epoch 8/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 838559.9375\n",
      "Epoch 9/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 792508.3750\n",
      "Epoch 10/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 852287.4375\n",
      "Epoch 11/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 965436.3750\n",
      "Epoch 12/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 752798.6875\n",
      "Epoch 13/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 856003.1250\n",
      "Epoch 14/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 865513.7500\n",
      "Epoch 15/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1009093.1250\n",
      "Epoch 16/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 792562.9375\n",
      "Epoch 17/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 848870.9375\n",
      "Epoch 18/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 834102.8125\n",
      "Epoch 19/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 805529.5625\n",
      "Epoch 20/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 862120.1875\n",
      "Epoch 21/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 803255.0625\n",
      "Epoch 22/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 733017.5000\n",
      "Epoch 23/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 834343.8750\n",
      "Epoch 24/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 794543.1250\n",
      "Epoch 25/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 746198.8750\n",
      "Epoch 26/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 786233.5000\n",
      "Epoch 27/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 820161.4375\n",
      "Epoch 28/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 810215.5625\n",
      "Epoch 29/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 805048.1875\n",
      "Epoch 30/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 860630.3125\n",
      "Epoch 31/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 752662.2500\n",
      "Epoch 32/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 767746.4375\n",
      "Epoch 33/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 740254.1250\n",
      "Epoch 34/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 710589.7500\n",
      "Epoch 35/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 714563.1250\n",
      "Epoch 36/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 730606.3125\n",
      "Epoch 37/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 722784.1875\n",
      "Epoch 38/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 736958.5625\n",
      "Epoch 39/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 776364.9375\n",
      "Epoch 40/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 783247.4375\n",
      "Epoch 41/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 729721.5625\n",
      "Epoch 42/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 753254.4375\n",
      "Epoch 43/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 716642.0625\n",
      "Epoch 44/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 734221.5625\n",
      "Epoch 45/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 789358.0625\n",
      "Epoch 46/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 718221.0625\n",
      "Epoch 47/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 725820.9375\n",
      "Epoch 48/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 714072.4375\n",
      "Epoch 49/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 707055.5625\n",
      "Epoch 50/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 782523.9375\n",
      "Epoch 51/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 682783.3125\n",
      "Epoch 52/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 686054.1875\n",
      "Epoch 53/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 660563.1875\n",
      "Epoch 54/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 706513.3125\n",
      "Epoch 55/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 824489.1250\n",
      "Epoch 56/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 777451.3125\n",
      "Epoch 57/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 740347.5625\n",
      "Epoch 58/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 752003.1250\n",
      "Epoch 59/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 698357.5000\n",
      "Epoch 60/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 639894.6250\n",
      "Epoch 61/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 781693.5000\n",
      "Epoch 62/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 751270.4375\n",
      "Epoch 63/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 721351.8125\n",
      "Epoch 64/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 696531.8750\n",
      "Epoch 65/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 833957.3750\n",
      "Epoch 66/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 809059.1250\n",
      "Epoch 67/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 672526.4375\n",
      "Epoch 68/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 742910.7500\n",
      "Epoch 69/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 772637.8750\n",
      "Epoch 70/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 782427.5625\n",
      "Epoch 71/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 704475.1875\n",
      "Epoch 72/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 763437.8125\n",
      "Epoch 73/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 753976.7500\n",
      "Epoch 74/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 670320.8750\n",
      "Epoch 75/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 650416.5625\n",
      "Epoch 76/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 665158.1875\n",
      "Epoch 77/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 703350.0625\n",
      "Epoch 78/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 687783.1875\n",
      "Epoch 79/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 724974.1250\n",
      "Epoch 80/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 784474.7500\n",
      "Epoch 81/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 717091.1250\n",
      "Epoch 82/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 700395.5625\n",
      "Epoch 83/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 677386.0625\n",
      "Epoch 84/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 698034.8125\n",
      "Epoch 85/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 792888.6875\n",
      "Epoch 86/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 642638.0000\n",
      "Epoch 87/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 606574.0000\n",
      "Epoch 88/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 706371.6875\n",
      "Epoch 89/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 639408.2500\n",
      "Epoch 90/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 649451.8125\n",
      "Epoch 91/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 664562.6875\n",
      "Epoch 92/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 690648.0000\n",
      "Epoch 93/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 737468.5000\n",
      "Epoch 94/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 822369.3750\n",
      "Epoch 95/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 702078.9375\n",
      "Epoch 96/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 661793.5625\n",
      "Epoch 97/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 800390.5000\n",
      "Epoch 98/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 871627.8125\n",
      "Epoch 99/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 718132.5625\n",
      "Epoch 100/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 651433.3125\n",
      "Epoch 101/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 758000.4375\n",
      "Epoch 102/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 658731.7500\n",
      "Epoch 103/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 700085.1875\n",
      "Epoch 104/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 700760.8125\n",
      "Epoch 105/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 637814.9375\n",
      "Epoch 106/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 744383.6875\n",
      "Epoch 107/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 674150.0000\n",
      "Epoch 108/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 741951.0625\n",
      "Epoch 109/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 689410.8750\n",
      "Epoch 110/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 738532.7500\n",
      "Epoch 111/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 660247.3125\n",
      "Epoch 112/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 686473.2500\n",
      "Epoch 113/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 671730.0625\n",
      "Epoch 114/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 599269.9375\n",
      "Epoch 115/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 644441.0000\n",
      "Epoch 116/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 740648.3750\n",
      "Epoch 117/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 776742.6875\n",
      "Epoch 118/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 732099.9375\n",
      "Epoch 119/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 697669.5625\n",
      "Epoch 120/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 761853.7500\n",
      "Epoch 121/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 675090.1875\n",
      "Epoch 122/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 729833.1875\n",
      "Epoch 123/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 786075.0625\n",
      "Epoch 124/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 869925.8125\n",
      "Epoch 125/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 654866.0000\n",
      "Epoch 126/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 877369.6875\n",
      "Epoch 127/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 676803.5000\n",
      "Epoch 128/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 705887.6250\n",
      "Epoch 129/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 638100.2500\n",
      "Epoch 130/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 858013.7500\n",
      "Epoch 131/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 731224.3125\n",
      "Epoch 132/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 641647.8750\n",
      "Epoch 133/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 769254.8125\n",
      "Epoch 134/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 743159.7500\n",
      "Epoch 135/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 689142.6250\n",
      "Epoch 136/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 630895.0625\n",
      "Epoch 137/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 697361.3125\n",
      "Epoch 138/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 696364.8125\n",
      "Epoch 139/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 653993.0625\n",
      "Epoch 140/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 749466.0625\n",
      "Epoch 141/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 679858.0000\n",
      "Epoch 142/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 761120.0000\n",
      "Epoch 143/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 711952.1250\n",
      "Epoch 144/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 783813.6875\n",
      "Epoch 145/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 685063.3125\n",
      "Epoch 146/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 689451.0000\n",
      "Epoch 147/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 728977.8750\n",
      "Epoch 148/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 672574.5000\n",
      "Epoch 149/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 717530.9375\n",
      "Epoch 150/150\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 734551.0000\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "X_numerical = data[['chieuDai', 'chieuNgang', 'dienTich', 'Phongngu', 'PhongTam', 'SoTang']].values\n",
    "\n",
    "categorical_columns = ['Loai', 'GiayTo', 'TinhTrangNoiThat', 'Phuong']\n",
    "for feature in categorical_columns:\n",
    "    data[feature] = data[feature].fillna('unknown') \n",
    "X_categorical_raw = data[categorical_columns].values\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_numerical = scaler.fit_transform(X_numerical)\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "X_categorical = encoder.fit_transform(X_categorical_raw)  \n",
    "\n",
    "y = data['Gia/m2'].to_numpy()  \n",
    "\n",
    "X_numerical_train, X_numerical_test, X_categorical_train, X_categorical_test, y_train, y_test = train_test_split(\n",
    "    X_numerical, X_categorical, y, test_size=0.2, random_state=1609\n",
    ")\n",
    "\n",
    "numerical_input = Input(shape=(6,), name='numerical_input')  \n",
    "x1 = Dense(6, activation='relu')(numerical_input)\n",
    "x1 = Dense(3, activation='relu')(x1)\n",
    "\n",
    "categorical_input = Input(shape=(X_categorical.shape[1],), name='categorical_input') \n",
    "x2 = Dense(24, activation='relu')(categorical_input)\n",
    "x2 = Dense(12, activation='relu')(x2)\n",
    "\n",
    "merged = Concatenate()([x1, x2])\n",
    "merged = Dense(64, activation='relu')(merged)\n",
    "merged = Dense(32, activation='relu')(merged)\n",
    "output = Dense(1)(merged)\n",
    "\n",
    "model = Model(inputs=[numerical_input, categorical_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.fit([X_numerical_train, X_categorical_train], y_train, epochs=150, batch_size=32)\n",
    "\n",
    "predictions = model.predict([X_numerical_test, X_categorical_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 599.8518335745823\n",
      "Mean Squared Error (MSE): 740085.7152216632\n",
      "R² Score: 0.49656666385038906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
